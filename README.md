
# Transformer_from_scratch
Implementation of "Attention Is All You Need".
"Attention Is All You Need" is a seminal paper in the field of natural language processing (NLP) and deep learning, proposing the Transformer architecture for sequence-to-sequence tasks. Published by researchers at Google in 2017,
Inspired from Umar Jamil.\\
Here are the attention visualization for English to Russian teanslation
![attention_visualization](https://github.com/Fariaz99cuet/Transformer_from_scratch/assets/103488785/a5e39208-91af-4eb3-8609-96faaeec5a78)

![attention_visualization_single png](https://github.com/Fariaz99cuet/Transformer_from_scratch/assets/103488785/a2b5b8b7-cb48-494d-9d4a-ae6953172538)


